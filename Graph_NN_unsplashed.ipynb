{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f1553-bc6f-491d-922f-eaac2e4c86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803d889-9615-439b-ae8e-43ae0c1e5a86",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c1dd7-fd4d-49cc-b158-289882e4aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from prod.engine_utils import * \n",
    "\n",
    "def cohort_grouper(df, group_key='cohort_date', \n",
    "                   target_variables=['active_users_users'],\\\n",
    "                   group_statics=['dx'], \n",
    "                   group_freq='W', group_function=np.sum):\n",
    "    return df.groupby([pd.Grouper(key=group_key, freq=group_freq)] + group_statics)[target_variables]\\\n",
    "             .agg(group_function)\\\n",
    "             .reset_index()\n",
    "\n",
    "def remove_nans_from_array(y):\n",
    "    return y[~np.isnan(y)]\n",
    "\n",
    "def count_nons_nans_in_array(y):\n",
    "    return np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "def nan_padding(x, target_length):\n",
    "    # padding sequences with nans in order to speed up computation with batches\n",
    "    # use with caution and propagating nans\n",
    "    return x + [np.nan] * (target_length - len(x))\n",
    "\n",
    "def init_nans(shapes):\n",
    "    x = np.zeros(shapes)\n",
    "    x[:] = np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7140a6b-5274-4670-99cf-505f5e6943f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='clf_data/clf_sparse_retention_dataset.parquet'#clf_parquet_thomas_sample_cohortised_aprdau.parquet'#\n",
    "raw_data = pd.read_parquet(filename)\n",
    "\n",
    "#.query(\"country=='US' and platform=='ANDROID' and channel=='organic'\")\\#\n",
    "#.pipe(cohort_grouper, target_variables=['active_users'])\\\n",
    "segment_data = raw_data\\\n",
    ".query(\"country=='US' and platform=='ANDROID' and channel=='organic'\")\\\n",
    ".dropna()\\\n",
    ".pipe(calculate_days_since_install)\\\n",
    ".pipe(format_date_column, column='cohort_date')\\\n",
    ".pipe(calculate_activity_date)\n",
    "segment_data['dx']=segment_data['dx'].astype(int)\n",
    "\n",
    "active_users_df = segment_data[['calendar_date','cohort_date','dx','active_users']].reset_index(drop=True).copy()\n",
    "cohort_size_df = segment_data.query('dx==0')[['cohort_date','active_users']]\\\n",
    "                           .rename({'active_users':'cohort_size'},axis=1)\\\n",
    "                           .reset_index(drop=True).copy()\n",
    "\n",
    "\n",
    "full_domain_dimensions = generate_ranged_clf_dataframe(\n",
    "                start_date=segment_data['cohort_date'].min(),#config_nn['start_input_date'],\n",
    "                end_date=segment_data['cohort_date'].max()#config_nn['end_forecast_date'],\n",
    "            )\n",
    "\n",
    "actual_input = (\n",
    "    full_domain_dimensions[['cohort_date','dx']]\n",
    "    .merge(active_users_df, on=['cohort_date','dx'], how='left')\n",
    "    .merge(cohort_size_df, on=['cohort_date'], how='left')\n",
    "    .assign(retention=lambda x: x['active_users']/x['cohort_size'])\n",
    "    .fillna({\"cohort_size\": 0,\"retention\": 0,\"active_users\": 0})\n",
    "    .pipe(calculate_activity_date)\n",
    "    .query(\"dx > 0\")\n",
    "    .query(\"dx < 300\")\n",
    "    .query(\"cohort_size > 3\")\n",
    "    .assign(\n",
    "                lag1_cohort_date=lambda df: df.groupby([\"dx\"])[\"retention\"].shift(1),\n",
    "                lag7_cohort_date=lambda df: df.groupby([\"dx\"])[\"retention\"].shift(7),\n",
    "                lag1_dx=lambda df: df.groupby([\"cohort_date\"])[\"retention\"].shift(1),\n",
    "                day_of_week_sin = lambda df: np.sin(df['calendar_date'].dt.weekday * (2 * np.pi / 7)),\n",
    "                day_of_week_cos = lambda df: np.cos(df['calendar_date'].dt.weekday * (2 * np.pi / 7))\n",
    "            )\n",
    "            .assign(\n",
    "                lag1_cohort_date=lambda df: np.where(\n",
    "                    df[\"lag1_cohort_date\"].isna(),\n",
    "                    df[\"retention\"],\n",
    "                    df[\"lag1_cohort_date\"],\n",
    "                ),\n",
    "                lag7_cohort_date=lambda df: np.where(\n",
    "                    df[\"lag7_cohort_date\"].isna(),\n",
    "                    df[\"retention\"],\n",
    "                    df[\"lag7_cohort_date\"],\n",
    "                ),\n",
    "                lag1_dx=lambda df: np.where(\n",
    "                    df[\"lag1_dx\"].isna(), df[\"retention\"], df[\"lag1_dx\"]\n",
    "                ),\n",
    "                log_dnu=lambda df: np.log(df[\"cohort_size\"]),\n",
    "                log_dx=lambda df: np.log(df[\"dx\"]),\n",
    "            )\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index().copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43289bef-976d-46ed-aaa5-30b24899f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_input.cohort_date.min(), actual_input.cohort_date.max()\n",
    "train_test_sep_date = pd.to_datetime('2022-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134ec00-fe76-4200-b0b7-5eb02f4b9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_domain = generate_ranged_clf_dataframe(\n",
    "            start_date=actual_input.cohort_date.min(),\n",
    "            end_date=train_test_sep_date,\n",
    "        ).assign(domain = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b913a-121c-4985-93e1-bbab65c1da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_input = actual_input.merge(train_domain, on=['cohort_date','dx'], how='left')\n",
    "actual_input['domain'] = actual_input['domain'].fillna('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ced6848-ef2b-4ae1-814a-b3bb07433a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pivot_df = actual_input.pivot(index='cohort_date', columns='dx',values='index')\n",
    "nodes_pivot = nodes_pivot_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13ade8b-26b5-4de1-8a4b-e897913b7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neighbour = init_nans(nodes_pivot.shape)\n",
    "left_neighbour = init_nans(nodes_pivot.shape)\n",
    "for i in range(nodes_pivot.shape[0]):\n",
    "    for j in range(nodes_pivot.shape[1]):\n",
    "        if j != 0:\n",
    "            top_neighbour[i,j] = nodes_pivot[i,j-1]\n",
    "        if i != 0:\n",
    "            left_neighbour[i,j] = nodes_pivot[i-1,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3dba7-afb5-4fa6-a3c0-f370209df658",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_neighbour_df = pd.DataFrame(top_neighbour, index=nodes_pivot_df.index, columns=nodes_pivot_df.columns)\n",
    "left_neighbour_df = pd.DataFrame(left_neighbour, index=nodes_pivot_df.index, columns=nodes_pivot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c4a3a-9c23-4362-a8c0-ee66784bfea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_edges = (\n",
    "nodes_pivot_df\n",
    "    .unstack()\n",
    "    .to_frame()\n",
    "    .merge(top_neighbour_df\n",
    "           .unstack()\n",
    "           .to_frame(), left_index=True, right_index=True, suffixes=('_node','_top'))\n",
    "    .dropna()\n",
    "    .merge(actual_input[['index','retention','domain']], left_on='0_top', right_on='index')\n",
    "    [['0_top','0_node','retention','domain']]\n",
    ")\n",
    "\n",
    "left_edges = (\n",
    "nodes_pivot_df\n",
    "    .unstack()\n",
    "    .to_frame()\n",
    "    .merge(left_neighbour_df\n",
    "           .unstack()\n",
    "           .to_frame(), left_index=True, right_index=True, suffixes=('_node','_left'))\n",
    "    .dropna()\n",
    "    .merge(actual_input[['index','retention','domain']], left_on='0_left', right_on='index')\n",
    "    [['0_left','0_node','retention','domain']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1604e65b-933f-4974-9f12-bf74da329677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import TUDataset, GNNBenchmarkDataset, Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "from torch_geometric.nn import Sequential, GCNConv, global_mean_pool, GATConv\n",
    "from torch.nn import Dropout, Linear, ReLU, Sigmoid\n",
    "from torch import nn \n",
    "#from torchmetrics.functional import accuracy, precision_recall, confusion_matrix\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb01a3-8dbd-4ebc-84e6-ed3fdbe809af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain_data(domain):\n",
    "    edges_arr = np.concatenate((top_edges[top_edges.domain==domain][['0_top','0_node','retention']].values, \n",
    "                                left_edges[left_edges.domain==domain][['0_left','0_node','retention']].values))\n",
    "    target_arr = actual_input[actual_input.domain==domain]['retention'].values\n",
    "    \n",
    "    features_list = [\n",
    "            \"log_dnu\",\n",
    "            \"log_dx\",\n",
    "            \"lag1_cohort_date\",\n",
    "            \"lag7_cohort_date\",\n",
    "            \"lag1_dx\",\n",
    "            \"day_of_week_sin\",\n",
    "            \"day_of_week_cos\",\n",
    "        ]\n",
    "    features_arr = actual_input[actual_input.domain==domain][features_list].values\n",
    "    return edges_arr, target_arr, features_arr\n",
    "\n",
    "def get_data():\n",
    "    edges_arr = np.concatenate((top_edges[['0_top','0_node']].values, \n",
    "                                left_edges[['0_left','0_node']].values))\n",
    "    target_arr = actual_input['retention'].values\n",
    "    \n",
    "    features_list = [\n",
    "            \"log_dnu\",\n",
    "            \"log_dx\",\n",
    "            \"lag1_cohort_date\",\n",
    "            \"lag7_cohort_date\",\n",
    "            \"lag1_dx\",\n",
    "            \"day_of_week_sin\",\n",
    "            \"day_of_week_cos\",\n",
    "        ]\n",
    "    features_arr = actual_input[features_list].values\n",
    "    return edges_arr, target_arr, features_arr\n",
    "\n",
    "#train_edges, train_target, train_features = get_data('train')\n",
    "#test_edges, test_target, test_features = get_data('test')\n",
    "#convert_indices = dict(zip(np.unique(train_edges[:,:2]).astype(int), np.arange(0, len(np.unique(train_edges[:,:2])))))\n",
    "#new_train_edges = np.zeros_like(train_edges[:,:2])\n",
    "#for i in range(new_train_edges.shape[0]):\n",
    "#    for j in range(new_train_edges.shape[1]):\n",
    "#        new_train_edges[i,j] = convert_indices[train_edges[i,j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b5cff1-5c61-48f6-bcc5-4c67db97f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_arr, target_arr, features_arr = get_data()\n",
    "\n",
    "x = torch.from_numpy(features_arr).float()\n",
    "y = torch.from_numpy(target_arr).unsqueeze(1).float()\n",
    "edge_index = torch.from_numpy(edges_arr).t().to(torch.int64)\n",
    "\n",
    "# building the Data graph\n",
    "retention_graph_data = Data(x=x, edge_index=edge_index, y=y) \n",
    "\n",
    "spliter = RandomNodeSplit(split='train_rest',\n",
    "                num_val=0.0,\n",
    "                num_test=0.0\n",
    "               )\n",
    "\n",
    "retention_graph_data = spliter(retention_graph_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc7989-7b89-40b2-9485-ecd80eddf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetentionGCN(torch.nn.Module):\n",
    "    def __init__(self, n_features, latent_layer_1=16, latent_layer_2=4):\n",
    "        super().__init__()\n",
    "        \"\"\" GCNConv layers \"\"\"\n",
    "        self.latent_layer_1 = latent_layer_1\n",
    "        self.latent_layer_2 = latent_layer_2\n",
    "        \n",
    "        self.n_features = n_features\n",
    "        \n",
    "        self.model = Sequential(\"x, edge_index\",\n",
    "                                [\n",
    "                                    (GCNConv(self.n_features, self.latent_layer_1), \"x, edge_index -> h1\"),\n",
    "                                    (ReLU(), \"h1 -> r1\"),\n",
    "                                    (Dropout(p=0.2), \"r1 -> d1\"),\n",
    "                                    (GCNConv(self.latent_layer_1, self.latent_layer_2), \"d1, edge_index -> h2\"),\n",
    "                                    (ReLU(), \"h2 -> r2\"),\n",
    "                                    (Dropout(p=0.2), \"r2 -> d2\"),\n",
    "                                    (Linear(self.latent_layer_2, 1), \"d2 -> a3\"),\n",
    "                                    (Sigmoid(), \"a3 -> x_output\")\n",
    "                                ]\n",
    "                               )\n",
    "        #self.conv1 = GCNConv(self.n_features, self.latent_layer_1)\n",
    "        #self.relu1 = ReLU()\n",
    "        #self.dropout1 = Dropout(p=0.2)\n",
    "        \n",
    "        #self.conv2 = GCNConv(self.latent_layer_1, self.latent_layer_2)\n",
    "        \n",
    "        #self.layer_out = Linear(self.latent_layer_2, 1) \n",
    "        #self.sigmoid = Sigmoid()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        \n",
    "        x_output = self.model(x, edge_index)\n",
    "\n",
    "        return x_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad8efe-9c66-4a58-9517-c71906d00d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader, NeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a08818-9ecf-49a5-aa5d-80cc22ce0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NeighborLoader(\n",
    "    retention_graph_data,\n",
    "    # Sample 30 neighbors for each node for 2 iterations\n",
    "    num_neighbors=[5] * 2,\n",
    "    # Use a batch size of 128 for sampling training nodes\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dccfd5-4a4b-4f95-8bc4-7fd5efa1a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RetentionGCN(retention_graph_data.num_features)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_function = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47bbb8-209c-4da3-ac0c-b3c93b698d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labelled_mask = retention_graph_data.train_mask\n",
    "test_labelled_mask = retention_graph_data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f35e2b-9d81-4f4a-8c8b-e25f6d9bc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828dbfec-0ee3-4c3d-a134-797c073aca8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.train()\n",
    "losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(40):\n",
    "    epoch_loss = 0\n",
    "    epoch_test_loss = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        xtrain = data.x[data.train_mask]\n",
    "        edge_train = data.edge_index\n",
    "        out = model(xtrain, edge_train).view(-1) # flatten\n",
    "\n",
    "        loss = loss_function(out, \n",
    "                             data.y[data.train_mask].float().view(-1),\n",
    "                            )\n",
    "    \n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # evaluation metrics\n",
    "        \n",
    "        xtest = data.x[data.test_mask]\n",
    "        edge_test = data.edge_index\n",
    "        #test_loss = F.mse_loss(model(xtest, edge_test), data.y[data.test_mask])\n",
    "        #epoch_test_loss += test_loss.item()\n",
    "        \n",
    "    losses.append(loss.item())\n",
    "    # take first element of each eval metric to get the metric of class 0 (illicit transactions)\n",
    "    train_accuracies.append(epoch_loss)\n",
    "    #test_accuracies.append(epoch_test_loss)\n",
    "    \n",
    "    print(epoch+1, \n",
    "      \"{:.2f}\".format(epoch_loss), \n",
    "      \"{:.2f}\".format(epoch_test_loss),\n",
    "      sep='\\t')\n",
    "\n",
    "    # early stopping\n",
    "    if len(test_accuracies) > 20:\n",
    "        if np.mean(test_accuracies[-4:]) > np.mean(test_accuracies[-5:-1]): # little moving average on 4 consecutive epochs to be sure it's globally goign up\n",
    "            print('\\nEarly stopping')\n",
    "            print(f\"Epoch {epoch}, loss {epoch_loss}, test_loss {epoch_test_loss}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69763cec-8659-45d1-b3fd-1c678b5a7705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = []\n",
    "n_ids = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for data in loader:\n",
    "        xtrain = data.x[data.train_mask]\n",
    "        out = model(xtrain, edge_train).view(-1) # flatten\n",
    "        output.append(out)\n",
    "        n_ids.append(data.n_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b9ea3-d6b3-454e-9d2b-8b30af65f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [o for out in output for o in out.detach().tolist()]\n",
    "nodes = [o for out in n_ids for o in out.detach().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b1468-fdc0-4bef-a436-da021b4ec9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(zip(outputs,nodes), columns=['ypred','nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d3eef1-48c1-4375-8135-969959fed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pivot_df.unstack().dropna().reset_index()[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592b0a8-d06a-43e5-98bb-ab3ebc07eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "852908/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7817d5a-a671-42b2-aeff-87686c3c2836",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[results.nodes==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f747f3ab-5f78-4f40-8a30-fe8f18b5c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_pivot_df.unstack().reset_index().merge(results, left_on=0,right_on='nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfe6ceae-7f91-4175-906a-2beac7d62142",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unnested_list_of_tensor(list_of_tensors):\n",
    "    return [element for _tensor in list_of_tensors for element in _tensor.detach().tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8816db26-f84c-4598-aa97-9d43efac8cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ba71c5-3e25-47db-bdfe-1358a73ac011",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [torch.tensor([1,2,3,4,5]), torch.tensor([1,2,3,4,5]), torch.tensor([1,2,3,4,5]), torch.tensor([1,2,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9221c1d8-0311-4816-ab51-60c59c1703d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnested_list_of_tensor(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c028c62b-0715-43a2-87b9-acce1fea08d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDPModel(torch.nn.Module):\n",
    "    def __init__(self, num_features=2, hidden_size=8, target_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_features = num_features\n",
    "        self.target_size = target_size\n",
    "        self.convs = [GATConv(self.num_features, self.hidden_size, edge_dim = 1),\n",
    "                      GATConv(self.hidden_size, self.hidden_size, edge_dim = 1)]\n",
    "        self.linear = nn.Linear(self.hidden_size, self.target_size)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index, edge_attr=edge_attr) # adding edge features here!\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index, edge_attr=edge_attr) # edge features here as well\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return F.sigmoid(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add780b9-09c4-4665-8582-f0790fda5b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, name_prefix, hyperparams):\n",
    "    ''' \n",
    "    Train model with given hyperparams dict.\n",
    "    Saves the following CSVs over the course of training:\n",
    "    1. the loss trajectory: the val and train loss every save_loss_interval epochs at\n",
    "       filename 'results/{name_prefix}_{learning_rate}_train.csv' e.g. 'results/baseline_0.05_train.csv'\n",
    "    2. every save_model_interval save both the model at e.g. 'models/baseline_0.05_0_out_of_1000.pt`\n",
    "       and the predicted values vs actual values in `results/baseline_0.05_0_out_of_1000_prediction.csv' on the test data.\n",
    "    '''\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    n_epochs = hyperparams['n_epochs']\n",
    "    save_loss_interval = hyperparams['save_loss_interval']\n",
    "    print_interval = hyperparams['print_interval']\n",
    "    save_model_interval = hyperparams['save_model_interval']\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    loader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    losses = []\n",
    "    test_data = data_test[0]\n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for data in loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.mse_loss(out, data.y)\n",
    "            epoch_loss += loss.item() \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % save_loss_interval == 0:\n",
    "            val_loss = evaluate_model(model, data_val) / NUM_VAL\n",
    "            train_loss = epoch_loss / NUM_TRAIN * batch_size\n",
    "            if epoch % print_interval == 0:\n",
    "                print(\"Epoch: {} Train loss: {:.2e} Validation loss: {:.2e}\".format(epoch, train_loss, val_loss))\n",
    "            losses.append((epoch, train_loss, val_loss))\n",
    "        if epoch % save_model_interval == 0:\n",
    "            # save predictions for plotting\n",
    "            model.eval()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019dc99-0e6d-4c9e-9ff0-89a810d6e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'batch_size' : 3, \n",
    "    'save_loss_interval' : 10, \n",
    "    'print_interval' : 50,\n",
    "    'save_model_interval' : 250,\n",
    "    'n_epochs' : 1500,\n",
    "    'learning_rate' : 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87debffe-0b7c-4e17-b813-a021b987c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "model = GDPModel().float() # needs to be double precision\n",
    "model_loss_traj = train(model, \"model\", hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec090b52-7a5a-4474-884c-a2d917be39f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b8d2d3-c037-4ec2-8d8f-cf4c3672651e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc8e2e3-0b7d-42c5-86ed-27fca22b7271",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c5e25-3f57-48f7-9e76-3063d10f4b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605721f-98bf-421c-b5cf-b2ab3823d6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce2330-3871-4d9c-9742-8175a2b8142b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99c1cb-214f-427c-81f6-cf5daa77e3cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b10151e-a06d-4eb1-b10d-07b7294d279c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4f575-7032-44bb-9a91-20a641124b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aa70f-fc9a-4629-b180-683ace1098a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
