{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f1553-bc6f-491d-922f-eaac2e4c86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e803d889-9615-439b-ae8e-43ae0c1e5a86",
   "metadata": {},
   "source": [
    "### Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c1dd7-fd4d-49cc-b158-289882e4aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "from prod.engine_utils import * \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def cohort_grouper(df, group_key='cohort_date', \n",
    "                   target_variables=['active_users_users'],\\\n",
    "                   group_statics=['dx'], \n",
    "                   group_freq='W', group_function=np.sum):\n",
    "    return df.groupby([pd.Grouper(key=group_key, freq=group_freq)] + group_statics)[target_variables]\\\n",
    "             .agg(group_function)\\\n",
    "             .reset_index()\n",
    "\n",
    "def remove_nans_from_array(y):\n",
    "    return y[~np.isnan(y)]\n",
    "\n",
    "def count_nons_nans_in_array(y):\n",
    "    return np.count_nonzero(~np.isnan(y))\n",
    "\n",
    "def nan_padding(x, target_length):\n",
    "    # padding sequences with nans in order to speed up computation with batches\n",
    "    # use with caution and propagating nans\n",
    "    return x + [np.nan] * (target_length - len(x))\n",
    "\n",
    "def init_nans(shapes):\n",
    "    x = np.zeros(shapes)\n",
    "    x[:] = np.nan\n",
    "    return x\n",
    "\n",
    "\n",
    "class FeatureEngineeringTransformer(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"Feature Engineering transform class for ML inputs\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.encoding_variables = {}\n",
    "\n",
    "    def fit_transform(self, df):\n",
    "        \"\"\" Fit-transform function applied to training (input) domain\n",
    "        \"\"\"\n",
    "        \n",
    "        self.encoding_variables['ohe_col_names'] = []\n",
    "        if \"country\" in df:\n",
    "            ohe_country_encoder = OneHotEncoder(sparse=False)\n",
    "            encoded_array = ohe_country_encoder.fit_transform(\n",
    "                df.loc[:, [\"country\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=ohe_country_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "            self.ohe_country_encoder = ohe_country_encoder\n",
    "            self.encoding_variables['ohe_col_names'] += list(ohe_country_encoder.get_feature_names_out())\n",
    "\n",
    "        if \"channel\" in df:\n",
    "            ohe_channel_encoder = OneHotEncoder(sparse=False)\n",
    "            encoded_array = ohe_channel_encoder.fit_transform(\n",
    "                df.loc[:, [\"channel\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=ohe_channel_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "            self.ohe_channel_encoder = ohe_channel_encoder\n",
    "\n",
    "            self.encoding_variables['ohe_col_names'] += list(ohe_channel_encoder.get_feature_names_out()) \n",
    "\n",
    "        if \"platform\" in df:\n",
    "            ohe_platform_encoder = OneHotEncoder(sparse=False)\n",
    "            encoded_array = ohe_platform_encoder.fit_transform(\n",
    "                df.loc[:, [\"platform\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=ohe_platform_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "            self.ohe_platform_encoder = ohe_platform_encoder\n",
    "\n",
    "            self.encoding_variables['ohe_col_names'] += list(ohe_platform_encoder.get_feature_names_out()) \n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"Feature transform function for forecast and extrapolate domain\n",
    "        \"\"\"\n",
    "        if \"country\" in df:\n",
    "            encoded_array = self.ohe_country_encoder.transform(\n",
    "                df.loc[:, [\"country\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=self.ohe_country_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "        if \"channel\" in df:\n",
    "            encoded_array = self.ohe_channel_encoder.transform(\n",
    "                df.loc[:, [\"channel\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=self.ohe_channel_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "        if \"platform\" in df:\n",
    "            encoded_array = self.ohe_platform_encoder.transform(\n",
    "                df.loc[:, [\"platform\"]]\n",
    "            )\n",
    "            df_encoded = pd.DataFrame(\n",
    "                encoded_array, columns=self.ohe_platform_encoder.get_feature_names_out()\n",
    "            )\n",
    "            df = pd.concat([df, df_encoded], axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "class RetNormalizer(TransformerMixin, BaseEstimator):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.encoding_variables = {}\n",
    "    \n",
    "    def _fit_transform_normalize_retention(self, df):\n",
    "        MAX_DX = 90\n",
    "\n",
    "        df['active_users'] = df['retention'] * df['cohort_size']\n",
    "        df['sq_au'] = df['active_users']**2\n",
    "\n",
    "        std_retention_df = np.sqrt(df[df['dx'] < MAX_DX].groupby('dx')['sq_au'].sum() / df[df['dx'] < MAX_DX].groupby('dx')['cohort_size'].sum())\n",
    "        avg_retention_df = (df[df['dx'] < MAX_DX].groupby('dx')['active_users'].sum() / df[df['dx'] < MAX_DX].groupby('dx')['cohort_size'].sum())\n",
    "\n",
    "        self.encoding_variables[\"std_retention_df\"] = std_retention_df\n",
    "        self.encoding_variables[\"avg_retention_df\"] = avg_retention_df\n",
    "\n",
    "        df = (\n",
    "            pd.merge(df, std_retention_df.rename(\"std_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"std_retention\": std_retention_df.values[-1]})\n",
    "            .merge(avg_retention_df.rename(\"avg_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"avg_retention\": avg_retention_df.values[-1]})\n",
    "            .eval(\"norm_retention = (retention - avg_retention) / std_retention\")\n",
    "        ) \n",
    "\n",
    "        return df.drop([\"sq_au\",\"std_retention\",\"avg_retention\"], axis=1)\n",
    "\n",
    "\n",
    "    def _transform_normalize_retention(self, df):\n",
    "        std_retention_df = self.encoding_variables[\"std_retention_df\"]\n",
    "        avg_retention_df = self.encoding_variables[\"avg_retention_df\"]\n",
    "\n",
    "        df = (\n",
    "            pd.merge(df, std_retention_df.rename(\"std_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"std_retention\": std_retention_df.values[-1]})\n",
    "            .merge(avg_retention_df.rename(\"avg_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"avg_retention\": avg_retention_df.values[-1]})\n",
    "            .eval(\"norm_retention = (retention - avg_retention) / std_retention\")\n",
    "        )\n",
    "\n",
    "        return df.drop([\"std_retention\",\"avg_retention\"], axis=1)\n",
    "\n",
    "\n",
    "    def get_retention_values(self, df):\n",
    "        std_retention_df = self.encoding_variables[\"std_retention_df\"]\n",
    "        avg_retention_df = self.encoding_variables[\"avg_retention_df\"]\n",
    "\n",
    "        df = (\n",
    "            pd.merge(df, std_retention_df.rename(\"std_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"std_retention\": std_retention_df.values[-1]})\n",
    "            .merge(avg_retention_df.rename(\"avg_retention\"), left_on='dx', right_index=True)\n",
    "            .fillna({\"avg_retention\": avg_retention_df.values[-1]})\n",
    "            .eval(\"pred_retention = ypred * std_retention + avg_retention\")\n",
    "        )\n",
    "\n",
    "        return df.drop([\"std_retention\",\"avg_retention\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db57c34-450c-451b-bbef-44c72e35b77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename ='clf_data/clf_sparse_retention_dataset.parquet'#clf_parquet_thomas_sample_cohortised_aprdau.parquet'#\n",
    "raw_data = pd.read_parquet(filename)\n",
    "\n",
    "#.query(\"country=='US' and platform=='ANDROID' and channel=='organic'\")\\#\n",
    "#.pipe(cohort_grouper, target_variables=['active_users'])\\\n",
    "segment_data = raw_data\\\n",
    ".dropna()\\\n",
    ".pipe(calculate_days_since_install)\\\n",
    ".pipe(format_date_column, column='cohort_date')\\\n",
    ".pipe(calculate_activity_date)\n",
    "segment_data['dx']=segment_data['dx'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60189e6-7b14-4767-964a-adc4a2e38922",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date, end_date = pd.to_datetime('2020-06-01'), pd.to_datetime('2022-01-01')\n",
    "full_dataset = pd.DataFrame()\n",
    "full_domain_dimensions = generate_ranged_clf_dataframe(\n",
    "                start_date=start_date,\n",
    "                end_date=end_date\n",
    ")\n",
    "\n",
    "\n",
    "for pl, ch, co in segment_data[['platform','channel','country']].drop_duplicates().values:\n",
    "    _segment = (\n",
    "        segment_data\n",
    "        .query(\"country==@co and platform==@pl and channel==@ch\")\n",
    "        .merge(full_domain_dimensions[['cohort_date','dx']], how='right')\n",
    "        .eval(\"country = @co\").eval(\"platform = @pl\").eval(\"channel = @ch\")\n",
    "    )\n",
    "    full_dataset = pd.concat((full_dataset, _segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45d44-ddc0-4765-9553-8360e80091f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = (\n",
    "    full_dataset\n",
    "    .pipe(calculate_activity_date)\n",
    "    .fillna({'active_users': 0})\n",
    "    .assign(\n",
    "        cohort_size = lambda df: df.groupby(['platform','channel','country','cohort_date'])['active_users'].transform(max)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7140a6b-5274-4670-99cf-505f5e6943f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_input = (\n",
    "    full_dataset\n",
    "    .assign(retention=lambda x: x['active_users']/x['cohort_size'])\n",
    "    .fillna({\"cohort_size\": 0,\"retention\": 0})\n",
    "    .pipe(calculate_activity_date)\n",
    "    .query(\"dx > 0\")\n",
    "    .query(\"dx < 300\")\n",
    "    .query(\"cohort_size > 3\")\n",
    "    .assign(\n",
    "        day_of_week_sin = lambda df: np.sin(df['calendar_date'].dt.weekday * (2 * np.pi / 7)),\n",
    "        day_of_week_cos = lambda df: np.cos(df['calendar_date'].dt.weekday * (2 * np.pi / 7)),\n",
    "        cohort_code = lambda df: (df.cohort_date - df.cohort_date.min()).dt.days,\n",
    "        norm_cohort_code = lambda df: df.cohort_code / df.cohort_code.max(),\n",
    "        calendar_code = lambda df: (df.calendar_date - df.calendar_date.min()).dt.days,\n",
    "        norm_calendar_code = lambda df: df.calendar_code / df.calendar_code.max(),\n",
    "        log_dnu = lambda df: np.log1p(df['cohort_size']),\n",
    "        log_dx = lambda df: np.log1p(df['dx']),\n",
    "        weight = lambda df: np.clip(np.sqrt(df['cohort_size']) / 20.0, 0.2, 1.2),\n",
    "    )\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134ec00-fe76-4200-b0b7-5eb02f4b9f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_train_date = end_date - pd.Timedelta(90,'D')\n",
    "train_domain = generate_ranged_clf_dataframe(\n",
    "            start_date=start_date,\n",
    "            end_date=end_train_date,\n",
    "        ).assign(domain = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b913a-121c-4985-93e1-bbab65c1da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = actual_input.merge(train_domain[['cohort_date','dx','domain']], on=['cohort_date','dx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3e4ed-8ed5-4c85-aa3b-bb3246df3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureEngineeringTransformer()\n",
    "training_input = fe.fit_transform(training_input)\n",
    "features = [\"norm_cohort_code\", \"norm_calendar_code\", \"log_dnu\", \"log_dx\", \"day_of_week_sin\", \"day_of_week_cos\"] + fe.encoding_variables['ohe_col_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d95f1-1e4e-44b9-81fa-7d74cf156983",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_norm = RetNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82765a-505f-431a-ad32-a2e03023ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = ret_norm._fit_transform_normalize_retention(training_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28eecb4-1363-4fb0-bbae-5ba707e259e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check xgboost version\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "xgb.set_config(verbosity=1)\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def gradient(predt: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the gradient squared log error.'''\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n",
    "            np.power(predt + 1, 2))\n",
    "\n",
    "def squared_log(predt: np.ndarray,\n",
    "                y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as\n",
    "    objective function.\n",
    "    '''\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    grad = gradient(predt, y)\n",
    "    hess = hessian(predt, y)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4f575-7032-44bb-9a91-20a641124b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(learning_rate = 0.3,\n",
    "                    n_estimators = 10, # large dataset so we can pump up the number of estimators\n",
    "                    max_depth = 6,\n",
    "                    min_child_weight = 1,\n",
    "                    subsample = 0.7,\n",
    "                    colsample_bytree = 0.7,\n",
    "                    objective=\"reg:squarederror\",#squared_log,#\n",
    "                    n_jobs = 2,\n",
    "                    verbosity = 1,\n",
    "                    eval_metric=['rmse','logloss'],\n",
    "                    seed = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85aa70f-fc9a-4629-b180-683ace1098a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_input[features].values\n",
    "y = training_input['retention'].values#training_input['norm_retention']\n",
    "dtrain = xgb.DMatrix(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a2c67-1c25-4f1f-b698-87e581c843bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, eval_set=[(X,y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9607c8-4fbf-41b0-8cdb-aaab1894de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_train_date = end_date - pd.Timedelta(90,'D')\n",
    "forecast_domain = generate_ranged_clf_dataframe(\n",
    "            start_date=end_train_date + pd.Timedelta(1,'D'),\n",
    "            end_date=end_date,\n",
    "        ).assign(domain = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286d742-5c5b-41ae-94bf-b9c7f87808d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_input = actual_input.merge(forecast_domain[['cohort_date','dx','domain']], on=['cohort_date','dx'])\n",
    "forecast_input = fe.transform(forecast_input)\n",
    "#forecast_input = ret_norm._transform_normalize_retention(forecast_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f018685-b615-4c3c-8fef-b35217357f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_X, forecast_Y = forecast_input[features], forecast_input['retention'].to_frame()\n",
    "predicted_Y = model.predict(forecast_X)\n",
    "forecast_input['ypred'] = predicted_Y\n",
    "#forecast_input = ret_norm.get_retention_values(forecast_input)\n",
    "\n",
    "training_predicted_Y = model.predict(X)\n",
    "training_input['ypred'] = training_predicted_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad5691-88b8-4cd7-b5af-84f3aa49eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_uao = forecast_input.query(\"country=='US' and platform=='IOS' and channel=='organic'\").copy()\n",
    "_uao['cohort_date'] = _uao['cohort_date'].dt.date\n",
    "\n",
    "_train_uao = training_input.query(\"country=='US' and platform=='IOS' and channel=='organic'\").copy()\n",
    "_train_uao['cohort_date'] = _train_uao['cohort_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081147eb-41c0-4a34-839f-00d42fdcdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2,figsize=(18,10))\n",
    "lognorm = LogNorm(vmin=0.0001,vmax=0.2)\n",
    "sns.heatmap(_uao.pivot(index='cohort_date',columns='dx',values='retention'), ax=axes[0,0]);\n",
    "sns.heatmap(_uao.pivot(index='cohort_date',columns='dx',values='ypred'), ax=axes[0,1]);\n",
    "\n",
    "sns.heatmap(_train_uao.pivot(index='cohort_date',columns='dx',values='retention'), ax=axes[1,0]);\n",
    "sns.heatmap(_train_uao.pivot(index='cohort_date',columns='dx',values='ypred'), ax=axes[1,1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0378d2e1-cc4e-4945-a2ce-c47b37f948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_input = (\n",
    "    forecast_input\n",
    "    .assign(\n",
    "        actual = lambda df : df['retention'] * df['cohort_size'],\n",
    "        forecast = lambda df : df['ypred'] * df['cohort_size'],\n",
    "    )\n",
    ")\n",
    "\n",
    "training_input = (\n",
    "    training_input\n",
    "    .assign(\n",
    "        actual = lambda df : df['retention'] * df['cohort_size'],\n",
    "        forecast = lambda df : df['ypred'] * df['cohort_size'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082845d-3423-4d63-b875-d5d670ab9a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(forecast_input['forecast'].sum() - forecast_input['actual'].sum()) /  forecast_input['actual'].sum(),\\\n",
    "(training_input['forecast'].sum() - training_input['actual'].sum()) /  training_input['actual'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fbd61e-d553-4c9e-92ac-dbb3c4cb9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input['ypred'] = model.predict(training_input[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8c744-4b69-4479-838a-c55a25a5ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "_uao = training_input.query(\"country=='US' and platform=='IOS' and channel=='organic'\").copy()\n",
    "_uao['cohort_date'] = _uao['cohort_date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a314cfa-70ab-47ba-8c1c-5be990cfaf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(18,6))\n",
    "lognorm = LogNorm(vmin=0.0001,vmax=0.2)\n",
    "sns.heatmap(_uao.pivot(index='cohort_date',columns='dx',values='retention'), norm=lognorm, ax=axes[0]);\n",
    "sns.heatmap(_uao.pivot(index='cohort_date',columns='dx',values='ypred'), norm=lognorm, ax=axes[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fef1ec-7e08-42de-b557-6f927b846370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a4391-e07e-4fac-86ac-b8003e0e116f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
